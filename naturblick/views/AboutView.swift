//
// Copyright © 2023 Museum für Naturkunde Berlin.
// This code is licensed under MIT license (see LICENSE.txt for details)


import SwiftUI

struct AboutView: NavigatableView {
    var holder: ViewControllerHolder = ViewControllerHolder()
    var viewName: String? = "About"
    
    func configureNavigationItem(item: UINavigationItem) {
    }
    
    var body: some View {
        ScrollView {
            VStack {
                Text("**About Naturblick**\n\nIn an interdisciplinary team, we are researching and developing digital tools for the urban experience of nature at the Museum für Naturkunde Berlin. For example, we are testing the use of pattern recognition in species identification.\n\nWith our smartphone app Naturblick we support the direct experience of nature in the city. With a variety of functions, you can explore nature in the city, identify animals and plants, and save your observations. Also take a look at our [plattform](https://naturblick.museumfuernaturkunde.berlin/). There you will find, for example, a map with confirmed observations of other users and the section WissensWeiten with informative and inspiring content for all those who want to take a closer look at the nature around them.\n\n**Sound recognition**\n\nThe results of the automated species identification are generated by comparing them with data (in this case audio recordings of birds) with which the algorithm has been trained.\n\nThis means\n1) only species that have been trained can be recognized,\n2) the recognition works best when there is a high similarity to the recording situation and quality (e.g. microphone quality).\n\nThe Naturblick algorithm was trained with audio material from the animal voice archive of the Museum für Naturkunde Berlin, the collaborative online database Xeno-Canto, and verified Naturblick recordings. The list of species that can be identified can be found [here](https://naturblick.museumfuernaturkunde.berlin/speciesaudiorecognition/).\n\n**Image recognition**\n\nThe results of automated species identification are generated by comparing them to data (in this case photos of plants) with which the algorithm has been trained.\n\nThis means\n1) only species that have been trained can be recognized,\n2) the recognition works best when there is a high similarity to the photo situation and quality (e.g. plants in the wild).\n\nThe nature eye algorithm was trained with imagery from naturgucker, the LIFECLEF Challenges, and iNaturalist. The list of species that can be identified can be found [here](https://naturblick.museumfuernaturkunde.berlin/speciesimagerecognition/).")
                    .tint(Color.onSecondaryButtonPrimary)
                    .font(.nbBody1)
                    .padding()
                Button {
                    let deviceName = "ios"
                    let appVersion = UIApplication.appVersion
                    let survey = "https://survey.naturkundemuseum-berlin.de/de/Feedback%20Naturblick?device_name=\(deviceName)&version=\(appVersion)"
                    if let url = URL(string: survey) {
                        UIApplication.shared.open(url)
                    }
                } label: {
                    Text("Use our feedback form")
                        .button()
                        .padding()
                }.background(Color.onSecondaryButtonPrimary)
                    .padding()
                Button {
                    let email = "naturblick@mfn-berlin.de"
                    if let url = URL(string: "mailto:\(email)") {
                        UIApplication.shared.open(url)
                    }
                } label: {
                    Text("Send an email")
                        .button()
                        .padding()
                }.background(Color.onSecondaryButtonPrimary)
                    .padding()
            }
        }.foregroundColor(.onSecondaryHighEmphasis)
    }
}

struct AboutView_Previews: PreviewProvider {
    static var previews: some View {
        AboutView()
    }
}
